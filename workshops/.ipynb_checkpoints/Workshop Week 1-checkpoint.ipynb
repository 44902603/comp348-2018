{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simple Statistics and NLTK\n",
    "\n",
    "The following exercises use a portion of the Gutenberg corpus that is stored in the corpus dataset of NLTK. [The Project Gutenberg](http://www.gutenberg.org/) is a large collection of electronic books that are out of copyright. These books are free to download for reading, or for our case, for doing a little of corpus analysis.\n",
    "\n",
    "To obtain the list of files of NLTK's Gutenberg corpus, type the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/Phillip/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain all words in the entire Gutenberg corpus of NLTK, type the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenbergwords = nltk.corpus.gutenberg.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can find the total number of words, and the first 10 words (do not attempt to display all the words or your computer will freeze!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2621613"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gutenbergwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenbergwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find the words of just a selection of documents, as shown below. For more details of what information you can extract from this corpus, read the \"Gutenberg corpus\" section of the [NLTK book chapter 2](http://www.nltk.org/book_1ed/ch02.html), section 2.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "*Write Python code that prints the 10 most common words in each of the documents of the Gutenberg corpus. Can you identify any similarities among them?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt [(',', 11454), ('.', 6928), ('to', 5183), ('the', 4844), ('and', 4672), ('of', 4279), ('I', 3178), ('a', 3004), ('was', 2385), ('her', 2381)]\n",
      "austen-persuasion.txt [(',', 6750), ('the', 3120), ('to', 2775), ('.', 2741), ('and', 2739), ('of', 2564), ('a', 1529), ('in', 1346), ('was', 1330), (';', 1290)]\n",
      "austen-sense.txt [(',', 9397), ('to', 4063), ('.', 3975), ('the', 3861), ('of', 3565), ('and', 3350), ('her', 2436), ('a', 2043), ('I', 2004), ('in', 1904)]\n",
      "bible-kjv.txt [(',', 70509), ('the', 62103), (':', 43766), ('and', 38847), ('of', 34480), ('.', 26160), ('to', 13396), ('And', 12846), ('that', 12576), ('in', 12331)]\n",
      "blake-poems.txt [(',', 680), ('the', 351), ('.', 201), ('And', 176), ('and', 169), ('of', 131), ('I', 130), ('in', 116), ('a', 108), (\"'\", 104)]\n",
      "bryant-stories.txt [(',', 3481), ('the', 3086), ('and', 1873), ('.', 1817), ('to', 1165), ('a', 988), ('\"', 900), ('he', 872), ('of', 801), ('was', 706)]\n",
      "burgess-busterbrown.txt [('.', 823), (',', 822), ('the', 639), ('he', 562), ('and', 484), ('to', 426), (\"'\", 401), ('of', 326), ('that', 285), ('a', 275)]\n",
      "carroll-alice.txt [(',', 1993), (\"'\", 1731), ('the', 1527), ('and', 802), ('.', 764), ('to', 725), ('a', 615), ('I', 543), ('it', 527), ('she', 509)]\n",
      "chesterton-ball.txt [(',', 4547), ('the', 4523), ('.', 3589), ('of', 2529), ('and', 2488), ('a', 2184), ('\"', 1751), ('to', 1558), ('in', 1355), ('that', 1120)]\n",
      "chesterton-brown.txt [('the', 4321), (',', 4069), ('.', 2784), ('of', 2087), ('and', 2074), ('a', 2074), ('\"', 1461), ('to', 1378), ('in', 1205), ('was', 1141)]\n",
      "chesterton-thursday.txt [(',', 3488), ('the', 3291), ('.', 2717), ('a', 1713), ('of', 1710), ('and', 1568), ('\"', 1336), ('to', 1045), ('in', 888), ('I', 885)]\n",
      "edgeworth-parents.txt [(',', 15219), ('the', 7149), ('.', 6945), ('to', 5150), ('and', 4769), ('\"', 3880), ('of', 3730), ('I', 3656), (\"'\", 3293), ('a', 3017)]\n",
      "melville-moby_dick.txt [(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024), ('a', 4569), ('to', 4542), (';', 4072), ('in', 3916), ('that', 2982)]\n",
      "milton-paradise.txt [(',', 10198), ('and', 2799), ('the', 2505), (';', 2317), ('to', 1758), ('of', 1486), ('.', 1254), ('in', 1083), ('his', 986), ('with', 876)]\n",
      "shakespeare-caesar.txt [(',', 2204), ('.', 1296), ('I', 531), ('the', 502), (':', 499), ('and', 409), (\"'\", 384), ('to', 370), ('you', 342), ('of', 336)]\n",
      "shakespeare-hamlet.txt [(',', 2892), ('.', 1886), ('the', 860), (\"'\", 729), ('and', 606), ('of', 576), ('to', 576), (':', 565), ('I', 553), ('you', 479)]\n",
      "shakespeare-macbeth.txt [(',', 1962), ('.', 1235), (\"'\", 637), ('the', 531), (':', 477), ('and', 376), ('I', 333), ('of', 315), ('to', 311), ('?', 241)]\n",
      "whitman-leaves.txt [(',', 17713), ('the', 8814), ('and', 4797), ('of', 4127), ('I', 2932), (\"'\", 2362), ('to', 1930), ('-', 1774), ('.', 1769), ('in', 1714)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from nltk.corpus import gutenberg\n",
    "for fileid in gutenberg.fileids():\n",
    "    gutenberg_counter = collections.Counter(nltk.corpus.gutenberg.words(fileid))\n",
    "    print (fileid, gutenberg_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "*Find the words with length of at least 7 characters in the complete Gutenberg corpus.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAPTER',\n",
       " 'Woodhouse',\n",
       " 'handsome',\n",
       " 'comfortable',\n",
       " 'disposition',\n",
       " 'blessings',\n",
       " 'existence',\n",
       " 'distress',\n",
       " 'youngest',\n",
       " 'daughters']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenbergwords = nltk.corpus.gutenberg.words()\n",
    "b = [word for word in gutenbergwords if len(word) >= 7]\n",
    "b[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "*Find the 5 most frequent words that are longer than 7 characters and occur more than 7 times in the complete Gutenberg corpus.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ConcatenatedCorpusView' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-0e2626f6b2aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgutenbergwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgutenberg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgutenberg_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgutenbergwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgutenberg_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ConcatenatedCorpusView' object is not callable"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "gutenbergwords = nltk.corpus.gutenberg.words()\n",
    "gutenberg_counter = collections.Counter(len(gutenbergwords())<7)\n",
    "gutenberg_counter.most_common(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4\n",
    "*Find the average number of words across the documents of the NLTK Gutenberg corpus.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145645.16666666666"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.corpus.gutenberg.words()\n",
    "books = nltk.corpus.gutenberg.fileids()\n",
    "num_words = len(words)\n",
    "num_books = len(books)\n",
    "num_words/num_books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.5\n",
    "*Find the Gutenberg document that has the longest average word length.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document with largest average word length is milton-paradise.txt with word length 4.835734572682675\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.6\n",
    "*Find the 10 most frequent bigrams in the entire Gutenberg corpus.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', 'and'), 41294),\n",
       " (('of', 'the'), 18912),\n",
       " (('in', 'the'), 9793),\n",
       " ((\"'\", 's'), 9781),\n",
       " ((';', 'and'), 7559),\n",
       " (('and', 'the'), 6432),\n",
       " (('the', 'LORD'), 5964),\n",
       " ((',', 'the'), 5957),\n",
       " ((',', 'I'), 5677),\n",
       " ((',', 'that'), 5352)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg = nltk.corpus.gutenberg.words()\n",
    "count = collections.Counter(nltk.bigrams(gutenberg))\n",
    "count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.7\n",
    "*Find the most frequent bigram that begins with \"Moby\" in Herman Melville's \"Moby Dick\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('[', 'Moby'): 1,\n",
       "         ('Moby', 'Dick'): 83,\n",
       "         ('Dick', 'by'): 1,\n",
       "         ('by', 'Herman'): 1,\n",
       "         ('Herman', 'Melville'): 1,\n",
       "         ('Melville', '1851'): 1,\n",
       "         ('1851', ']'): 1,\n",
       "         (']', 'ETYMOLOGY'): 1,\n",
       "         ('ETYMOLOGY', '.'): 1,\n",
       "         ('.', '('): 56,\n",
       "         ('(', 'Supplied'): 2,\n",
       "         ('Supplied', 'by'): 2,\n",
       "         ('by', 'a'): 80,\n",
       "         ('a', 'Late'): 1,\n",
       "         ('Late', 'Consumptive'): 1,\n",
       "         ('Consumptive', 'Usher'): 1,\n",
       "         ('Usher', 'to'): 1,\n",
       "         ('to', 'a'): 111,\n",
       "         ('a', 'Grammar'): 1,\n",
       "         ('Grammar', 'School'): 1,\n",
       "         ('School', ')'): 1,\n",
       "         (')', 'The'): 2,\n",
       "         ('The', 'pale'): 1,\n",
       "         ('pale', 'Usher'): 1,\n",
       "         ('Usher', '--'): 1,\n",
       "         ('--', 'threadbare'): 1,\n",
       "         ('threadbare', 'in'): 1,\n",
       "         ('in', 'coat'): 1,\n",
       "         ('coat', ','): 8,\n",
       "         (',', 'heart'): 1,\n",
       "         ('heart', ','): 17,\n",
       "         (',', 'body'): 1,\n",
       "         ('body', ','): 19,\n",
       "         (',', 'and'): 2607,\n",
       "         ('and', 'brain'): 1,\n",
       "         ('brain', ';'): 6,\n",
       "         (';', 'I'): 96,\n",
       "         ('I', 'see'): 24,\n",
       "         ('see', 'him'): 19,\n",
       "         ('him', 'now'): 8,\n",
       "         ('now', '.'): 19,\n",
       "         ('.', 'He'): 178,\n",
       "         ('He', 'was'): 30,\n",
       "         ('was', 'ever'): 2,\n",
       "         ('ever', 'dusting'): 1,\n",
       "         ('dusting', 'his'): 1,\n",
       "         ('his', 'old'): 14,\n",
       "         ('old', 'lexicons'): 1,\n",
       "         ('lexicons', 'and'): 1,\n",
       "         ('and', 'grammars'): 1,\n",
       "         ('grammars', ','): 1,\n",
       "         (',', 'with'): 260,\n",
       "         ('with', 'a'): 233,\n",
       "         ('a', 'queer'): 8,\n",
       "         ('queer', 'handkerchief'): 1,\n",
       "         ('handkerchief', ','): 2,\n",
       "         (',', 'mockingly'): 1,\n",
       "         ('mockingly', 'embellished'): 1,\n",
       "         ('embellished', 'with'): 2,\n",
       "         ('with', 'all'): 44,\n",
       "         ('all', 'the'): 198,\n",
       "         ('the', 'gay'): 5,\n",
       "         ('gay', 'flags'): 1,\n",
       "         ('flags', 'of'): 1,\n",
       "         ('of', 'all'): 132,\n",
       "         ('the', 'known'): 4,\n",
       "         ('known', 'nations'): 1,\n",
       "         ('nations', 'of'): 2,\n",
       "         ('of', 'the'): 1847,\n",
       "         ('the', 'world'): 86,\n",
       "         ('world', '.'): 19,\n",
       "         ('He', 'loved'): 1,\n",
       "         ('loved', 'to'): 1,\n",
       "         ('to', 'dust'): 1,\n",
       "         ('dust', 'his'): 1,\n",
       "         ('old', 'grammars'): 1,\n",
       "         ('grammars', ';'): 1,\n",
       "         (';', 'it'): 56,\n",
       "         ('it', 'somehow'): 4,\n",
       "         ('somehow', 'mildly'): 1,\n",
       "         ('mildly', 'reminded'): 1,\n",
       "         ('reminded', 'him'): 2,\n",
       "         ('him', 'of'): 8,\n",
       "         ('of', 'his'): 371,\n",
       "         ('his', 'mortality'): 1,\n",
       "         ('mortality', '.'): 1,\n",
       "         ('.', '\"'): 557,\n",
       "         ('\"', 'While'): 2,\n",
       "         ('While', 'you'): 1,\n",
       "         ('you', 'take'): 3,\n",
       "         ('take', 'in'): 4,\n",
       "         ('in', 'hand'): 18,\n",
       "         ('hand', 'to'): 11,\n",
       "         ('to', 'school'): 2,\n",
       "         ('school', 'others'): 1,\n",
       "         ('others', ','): 9,\n",
       "         ('and', 'to'): 27,\n",
       "         ('to', 'teach'): 2,\n",
       "         ('teach', 'them'): 1,\n",
       "         ('them', 'by'): 6,\n",
       "         ('by', 'what'): 10,\n",
       "         ('what', 'name'): 1,\n",
       "         ('name', 'a'): 1,\n",
       "         ('a', 'whale'): 97,\n",
       "         ('whale', '-'): 114,\n",
       "         ('-', 'fish'): 22,\n",
       "         ('fish', 'is'): 3,\n",
       "         ('is', 'to'): 40,\n",
       "         ('to', 'be'): 320,\n",
       "         ('be', 'called'): 9,\n",
       "         ('called', 'in'): 2,\n",
       "         ('in', 'our'): 21,\n",
       "         ('our', 'tongue'): 1,\n",
       "         ('tongue', 'leaving'): 1,\n",
       "         ('leaving', 'out'): 1,\n",
       "         ('out', ','): 38,\n",
       "         (',', 'through'): 17,\n",
       "         ('through', 'ignorance'): 1,\n",
       "         ('ignorance', ','): 2,\n",
       "         (',', 'the'): 908,\n",
       "         ('the', 'letter'): 5,\n",
       "         ('letter', 'H'): 1,\n",
       "         ('H', ','): 1,\n",
       "         (',', 'which'): 217,\n",
       "         ('which', 'almost'): 2,\n",
       "         ('almost', 'alone'): 1,\n",
       "         ('alone', 'maketh'): 1,\n",
       "         ('maketh', 'the'): 3,\n",
       "         ('the', 'signification'): 1,\n",
       "         ('signification', 'of'): 1,\n",
       "         ('the', 'word'): 26,\n",
       "         ('word', ','): 17,\n",
       "         (',', 'you'): 171,\n",
       "         ('you', 'deliver'): 1,\n",
       "         ('deliver', 'that'): 1,\n",
       "         ('that', 'which'): 3,\n",
       "         ('which', 'is'): 29,\n",
       "         ('is', 'not'): 67,\n",
       "         ('not', 'true'): 2,\n",
       "         ('true', '.\"'): 1,\n",
       "         ('.\"', '--'): 76,\n",
       "         ('--', 'HACKLUYT'): 1,\n",
       "         ('HACKLUYT', '\"'): 1,\n",
       "         ('\"', 'WHALE'): 2,\n",
       "         ('WHALE', '.'): 7,\n",
       "         ('.', '...'): 11,\n",
       "         ('...', 'Sw'): 1,\n",
       "         ('Sw', '.'): 1,\n",
       "         ('.', 'and'): 2,\n",
       "         ('and', 'Dan'): 1,\n",
       "         ('Dan', '.'): 2,\n",
       "         ('.', 'HVAL'): 1,\n",
       "         ('HVAL', '.'): 1,\n",
       "         ('.', 'This'): 76,\n",
       "         ('This', 'animal'): 1,\n",
       "         ('animal', 'is'): 1,\n",
       "         ('is', 'named'): 2,\n",
       "         ('named', 'from'): 1,\n",
       "         ('from', 'roundness'): 1,\n",
       "         ('roundness', 'or'): 1,\n",
       "         ('or', 'rolling'): 1,\n",
       "         ('rolling', ';'): 1,\n",
       "         (';', 'for'): 124,\n",
       "         ('for', 'in'): 10,\n",
       "         ('in', 'Dan'): 1,\n",
       "         ('.', 'HVALT'): 2,\n",
       "         ('HVALT', 'is'): 1,\n",
       "         ('is', 'arched'): 1,\n",
       "         ('arched', 'or'): 1,\n",
       "         ('or', 'vaulted'): 1,\n",
       "         ('vaulted', '.\"'): 1,\n",
       "         ('--', 'WEBSTER'): 1,\n",
       "         ('WEBSTER', \"'\"): 2,\n",
       "         (\"'\", 'S'): 59,\n",
       "         ('S', 'DICTIONARY'): 2,\n",
       "         ('DICTIONARY', '\"'): 1,\n",
       "         ('...', 'It'): 2,\n",
       "         ('It', 'is'): 85,\n",
       "         ('is', 'more'): 13,\n",
       "         ('more', 'immediately'): 2,\n",
       "         ('immediately', 'from'): 1,\n",
       "         ('from', 'the'): 428,\n",
       "         ('the', 'Dut'): 1,\n",
       "         ('Dut', '.'): 1,\n",
       "         ('and', 'Ger'): 1,\n",
       "         ('Ger', '.'): 1,\n",
       "         ('.', 'WALLEN'): 1,\n",
       "         ('WALLEN', ';'): 1,\n",
       "         (';', 'A'): 1,\n",
       "         ('A', '.'): 13,\n",
       "         ('.', 'S'): 3,\n",
       "         ('S', '.'): 4,\n",
       "         ('.', 'WALW'): 1,\n",
       "         ('WALW', '-'): 1,\n",
       "         ('-', 'IAN'): 1,\n",
       "         ('IAN', ','): 1,\n",
       "         (',', 'to'): 224,\n",
       "         ('to', 'roll'): 1,\n",
       "         ('roll', ','): 3,\n",
       "         ('to', 'wallow'): 1,\n",
       "         ('wallow', '.\"'): 1,\n",
       "         ('--', 'RICHARDSON'): 1,\n",
       "         ('RICHARDSON', \"'\"): 1,\n",
       "         ('DICTIONARY', 'KETOS'): 1,\n",
       "         ('KETOS', ','): 1,\n",
       "         (',', 'GREEK'): 1,\n",
       "         ('GREEK', '.'): 1,\n",
       "         ('.', 'CETUS'): 1,\n",
       "         ('CETUS', ','): 1,\n",
       "         (',', 'LATIN'): 1,\n",
       "         ('LATIN', '.'): 1,\n",
       "         ('.', 'WHOEL'): 1,\n",
       "         ('WHOEL', ','): 1,\n",
       "         (',', 'ANGLO'): 1,\n",
       "         ('ANGLO', '-'): 1,\n",
       "         ('-', 'SAXON'): 1,\n",
       "         ('SAXON', '.'): 1,\n",
       "         ('HVALT', ','): 1,\n",
       "         (',', 'DANISH'): 1,\n",
       "         ('DANISH', '.'): 1,\n",
       "         ('.', 'WAL'): 1,\n",
       "         ('WAL', ','): 1,\n",
       "         (',', 'DUTCH'): 1,\n",
       "         ('DUTCH', '.'): 1,\n",
       "         ('.', 'HWAL'): 1,\n",
       "         ('HWAL', ','): 1,\n",
       "         (',', 'SWEDISH'): 1,\n",
       "         ('SWEDISH', '.'): 1,\n",
       "         ('.', 'WHALE'): 2,\n",
       "         ('WHALE', ','): 4,\n",
       "         (',', 'ICELANDIC'): 1,\n",
       "         ('ICELANDIC', '.'): 1,\n",
       "         (',', 'ENGLISH'): 1,\n",
       "         ('ENGLISH', '.'): 1,\n",
       "         ('.', 'BALEINE'): 1,\n",
       "         ('BALEINE', ','): 1,\n",
       "         (',', 'FRENCH'): 1,\n",
       "         ('FRENCH', '.'): 1,\n",
       "         ('.', 'BALLENA'): 1,\n",
       "         ('BALLENA', ','): 1,\n",
       "         (',', 'SPANISH'): 1,\n",
       "         ('SPANISH', '.'): 1,\n",
       "         ('.', 'PEKEE'): 2,\n",
       "         ('PEKEE', '-'): 2,\n",
       "         ('-', 'NUEE'): 4,\n",
       "         ('NUEE', '-'): 2,\n",
       "         ('NUEE', ','): 2,\n",
       "         (',', 'FEGEE'): 1,\n",
       "         ('FEGEE', '.'): 1,\n",
       "         (',', 'ERROMANGOAN'): 1,\n",
       "         ('ERROMANGOAN', '.'): 1,\n",
       "         ('.', 'EXTRACTS'): 1,\n",
       "         ('EXTRACTS', '('): 1,\n",
       "         ('a', 'Sub'): 3,\n",
       "         ('Sub', '-'): 5,\n",
       "         ('-', 'Sub'): 3,\n",
       "         ('-', 'Librarian'): 1,\n",
       "         ('Librarian', ').'): 1,\n",
       "         (').', 'It'): 1,\n",
       "         ('It', 'will'): 7,\n",
       "         ('will', 'be'): 40,\n",
       "         ('be', 'seen'): 32,\n",
       "         ('seen', 'that'): 7,\n",
       "         ('that', 'this'): 49,\n",
       "         ('this', 'mere'): 2,\n",
       "         ('mere', 'painstaking'): 1,\n",
       "         ('painstaking', 'burrower'): 1,\n",
       "         ('burrower', 'and'): 1,\n",
       "         ('and', 'grub'): 1,\n",
       "         ('grub', '-'): 1,\n",
       "         ('-', 'worm'): 2,\n",
       "         ('worm', 'of'): 1,\n",
       "         ('of', 'a'): 327,\n",
       "         ('a', 'poor'): 7,\n",
       "         ('poor', 'devil'): 4,\n",
       "         ('devil', 'of'): 4,\n",
       "         ('Sub', 'appears'): 1,\n",
       "         ('appears', 'to'): 3,\n",
       "         ('to', 'have'): 61,\n",
       "         ('have', 'gone'): 10,\n",
       "         ('gone', 'through'): 4,\n",
       "         ('through', 'the'): 91,\n",
       "         ('the', 'long'): 50,\n",
       "         ('long', 'Vaticans'): 1,\n",
       "         ('Vaticans', 'and'): 1,\n",
       "         ('and', 'street'): 1,\n",
       "         ('street', '-'): 2,\n",
       "         ('-', 'stalls'): 1,\n",
       "         ('stalls', 'of'): 1,\n",
       "         ('the', 'earth'): 12,\n",
       "         ('earth', ','): 19,\n",
       "         (',', 'picking'): 2,\n",
       "         ('picking', 'up'): 1,\n",
       "         ('up', 'whatever'): 1,\n",
       "         ('whatever', 'random'): 1,\n",
       "         ('random', 'allusions'): 1,\n",
       "         ('allusions', 'to'): 3,\n",
       "         ('to', 'whales'): 2,\n",
       "         ('whales', 'he'): 2,\n",
       "         ('he', 'could'): 21,\n",
       "         ('could', 'anyways'): 1,\n",
       "         ('anyways', 'find'): 1,\n",
       "         ('find', 'in'): 2,\n",
       "         ('in', 'any'): 31,\n",
       "         ('any', 'book'): 1,\n",
       "         ('book', 'whatsoever'): 1,\n",
       "         ('whatsoever', ','): 2,\n",
       "         (',', 'sacred'): 1,\n",
       "         ('sacred', 'or'): 1,\n",
       "         ('or', 'profane'): 1,\n",
       "         ('profane', '.'): 1,\n",
       "         ('.', 'Therefore'): 10,\n",
       "         ('Therefore', 'you'): 1,\n",
       "         ('you', 'must'): 28,\n",
       "         ('must', 'not'): 10,\n",
       "         ('not', ','): 26,\n",
       "         (',', 'in'): 402,\n",
       "         ('in', 'every'): 7,\n",
       "         ('every', 'case'): 1,\n",
       "         ('case', 'at'): 1,\n",
       "         ('at', 'least'): 55,\n",
       "         ('least', ','): 15,\n",
       "         (',', 'take'): 17,\n",
       "         ('take', 'the'): 15,\n",
       "         ('the', 'higgledy'): 1,\n",
       "         ('higgledy', '-'): 1,\n",
       "         ('-', 'piggledy'): 1,\n",
       "         ('piggledy', 'whale'): 1,\n",
       "         ('whale', 'statements'): 1,\n",
       "         ('statements', ','): 2,\n",
       "         (',', 'however'): 53,\n",
       "         ('however', 'authentic'): 1,\n",
       "         ('authentic', ','): 1,\n",
       "         ('in', 'these'): 23,\n",
       "         ('these', 'extracts'): 2,\n",
       "         ('extracts', ','): 1,\n",
       "         (',', 'for'): 181,\n",
       "         ('for', 'veritable'): 1,\n",
       "         ('veritable', 'gospel'): 1,\n",
       "         ('gospel', 'cetology'): 1,\n",
       "         ('cetology', '.'): 2,\n",
       "         ('.', 'Far'): 4,\n",
       "         ('Far', 'from'): 1,\n",
       "         ('from', 'it'): 21,\n",
       "         ('it', '.'): 171,\n",
       "         ('.', 'As'): 97,\n",
       "         ('As', 'touching'): 2,\n",
       "         ('touching', 'the'): 20,\n",
       "         ('the', 'ancient'): 6,\n",
       "         ('ancient', 'authors'): 1,\n",
       "         ('authors', 'generally'): 1,\n",
       "         ('generally', ','): 2,\n",
       "         (',', 'as'): 523,\n",
       "         ('as', 'well'): 36,\n",
       "         ('well', 'as'): 19,\n",
       "         ('as', 'the'): 204,\n",
       "         ('the', 'poets'): 1,\n",
       "         ('poets', 'here'): 1,\n",
       "         ('here', 'appearing'): 1,\n",
       "         ('appearing', ','): 1,\n",
       "         (',', 'these'): 25,\n",
       "         ('extracts', 'are'): 1,\n",
       "         ('are', 'solely'): 1,\n",
       "         ('solely', 'valuable'): 1,\n",
       "         ('valuable', 'or'): 1,\n",
       "         ('or', 'entertaining'): 1,\n",
       "         ('entertaining', ','): 1,\n",
       "         ('as', 'affording'): 1,\n",
       "         ('affording', 'a'): 1,\n",
       "         ('a', 'glancing'): 2,\n",
       "         ('glancing', 'bird'): 1,\n",
       "         ('bird', \"'\"): 3,\n",
       "         (\"'\", 's'): 1737,\n",
       "         ('s', 'eye'): 6,\n",
       "         ('eye', 'view'): 1,\n",
       "         ('view', 'of'): 12,\n",
       "         ('of', 'what'): 17,\n",
       "         ('what', 'has'): 3,\n",
       "         ('has', 'been'): 50,\n",
       "         ('been', 'promiscuously'): 1,\n",
       "         ('promiscuously', 'said'): 1,\n",
       "         ('said', ','): 33,\n",
       "         (',', 'thought'): 24,\n",
       "         ('thought', ','): 15,\n",
       "         (',', 'fancied'): 1,\n",
       "         ('fancied', ','): 2,\n",
       "         ('and', 'sung'): 1,\n",
       "         ('sung', 'of'): 1,\n",
       "         ('of', 'Leviathan'): 2,\n",
       "         ('Leviathan', ','): 14,\n",
       "         (',', 'by'): 140,\n",
       "         ('by', 'many'): 5,\n",
       "         ('many', 'nations'): 1,\n",
       "         ('nations', 'and'): 1,\n",
       "         ('and', 'generations'): 1,\n",
       "         ('generations', ','): 3,\n",
       "         (',', 'including'): 7,\n",
       "         ('including', 'our'): 1,\n",
       "         ('our', 'own'): 9,\n",
       "         ('own', '.'): 11,\n",
       "         ('.', 'So'): 116,\n",
       "         ('So', 'fare'): 1,\n",
       "         ('fare', 'thee'): 1,\n",
       "         ('thee', 'well'): 1,\n",
       "         ('well', ','): 22,\n",
       "         (',', 'poor'): 11,\n",
       "         ('Sub', ','): 1,\n",
       "         (',', 'whose'): 48,\n",
       "         ('whose', 'commentator'): 1,\n",
       "         ('commentator', 'I'): 1,\n",
       "         ('I', 'am'): 73,\n",
       "         ('am', '.'): 1,\n",
       "         ('.', 'Thou'): 14,\n",
       "         ('Thou', 'belongest'): 1,\n",
       "         ('belongest', 'to'): 1,\n",
       "         ('to', 'that'): 52,\n",
       "         ('that', 'hopeless'): 1,\n",
       "         ('hopeless', ','): 2,\n",
       "         (',', 'sallow'): 1,\n",
       "         ('sallow', 'tribe'): 1,\n",
       "         ('tribe', 'which'): 1,\n",
       "         ('which', 'no'): 3,\n",
       "         ('no', 'wine'): 1,\n",
       "         ('wine', 'of'): 2,\n",
       "         ('of', 'this'): 132,\n",
       "         ('this', 'world'): 19,\n",
       "         ('world', 'will'): 1,\n",
       "         ('will', 'ever'): 2,\n",
       "         ('ever', 'warm'): 1,\n",
       "         ('warm', ';'): 1,\n",
       "         (';', 'and'): 853,\n",
       "         ('and', 'for'): 26,\n",
       "         ('for', 'whom'): 1,\n",
       "         ('whom', 'even'): 1,\n",
       "         ('even', 'Pale'): 1,\n",
       "         ('Pale', 'Sherry'): 1,\n",
       "         ('Sherry', 'would'): 1,\n",
       "         ('would', 'be'): 49,\n",
       "         ('be', 'too'): 7,\n",
       "         ('too', 'rosy'): 1,\n",
       "         ('rosy', '-'): 1,\n",
       "         ('-', 'strong'): 1,\n",
       "         ('strong', ';'): 2,\n",
       "         (';', 'but'): 340,\n",
       "         ('but', 'with'): 18,\n",
       "         ('with', 'whom'): 3,\n",
       "         ('whom', 'one'): 1,\n",
       "         ('one', 'sometimes'): 1,\n",
       "         ('sometimes', 'loves'): 1,\n",
       "         ('loves', 'to'): 1,\n",
       "         ('to', 'sit'): 7,\n",
       "         ('sit', ','): 2,\n",
       "         ('and', 'feel'): 4,\n",
       "         ('feel', 'poor'): 1,\n",
       "         ('poor', '-'): 2,\n",
       "         ('-', 'devilish'): 1,\n",
       "         ('devilish', ','): 1,\n",
       "         (',', 'too'): 67,\n",
       "         ('too', ';'): 8,\n",
       "         ('and', 'grow'): 1,\n",
       "         ('grow', 'convivial'): 1,\n",
       "         ('convivial', 'upon'): 1,\n",
       "         ('upon', 'tears'): 1,\n",
       "         ('tears', ';'): 1,\n",
       "         ('and', 'say'): 6,\n",
       "         ('say', 'to'): 6,\n",
       "         ('to', 'them'): 34,\n",
       "         ('them', 'bluntly'): 1,\n",
       "         ('bluntly', ','): 1,\n",
       "         ('with', 'full'): 1,\n",
       "         ('full', 'eyes'): 1,\n",
       "         ('eyes', 'and'): 3,\n",
       "         ('and', 'empty'): 1,\n",
       "         ('empty', 'glasses'): 1,\n",
       "         ('glasses', ','): 2,\n",
       "         ('and', 'in'): 83,\n",
       "         ('in', 'not'): 4,\n",
       "         ('not', 'altogether'): 6,\n",
       "         ('altogether', 'unpleasant'): 1,\n",
       "         ('unpleasant', 'sadness'): 1,\n",
       "         ('sadness', '--'): 1,\n",
       "         ('--', 'Give'): 1,\n",
       "         ('Give', 'it'): 2,\n",
       "         ('it', 'up'): 11,\n",
       "         ('up', ','): 46,\n",
       "         (',', 'Sub'): 1,\n",
       "         ('-', 'Subs'): 1,\n",
       "         ('Subs', '!'): 1,\n",
       "         ('!', 'For'): 12,\n",
       "         ('For', 'by'): 4,\n",
       "         ('by', 'how'): 1,\n",
       "         ('how', 'much'): 6,\n",
       "         ('much', 'the'): 19,\n",
       "         ('the', 'more'): 46,\n",
       "         ('more', 'pains'): 1,\n",
       "         ('pains', 'ye'): 1,\n",
       "         ('ye', 'take'): 2,\n",
       "         ('take', 'to'): 7,\n",
       "         ('to', 'please'): 2,\n",
       "         ('please', 'the'): 1,\n",
       "         ('world', ','): 33,\n",
       "         ('by', 'so'): 6,\n",
       "         ('so', 'much'): 57,\n",
       "         ('more', 'shall'): 1,\n",
       "         ('shall', 'ye'): 1,\n",
       "         ('ye', 'for'): 2,\n",
       "         ('for', 'ever'): 42,\n",
       "         ('ever', 'go'): 3,\n",
       "         ('go', 'thankless'): 1,\n",
       "         ('thankless', '!'): 1,\n",
       "         ('!', 'Would'): 2,\n",
       "         ('Would', 'that'): 3,\n",
       "         ('that', 'I'): 77,\n",
       "         ('I', 'could'): 36,\n",
       "         ('could', 'clear'): 1,\n",
       "         ('clear', 'out'): 1,\n",
       "         ('out', 'Hampton'): 1,\n",
       "         ('Hampton', 'Court'): 1,\n",
       "         ('Court', 'and'): 1,\n",
       "         ('and', 'the'): 357,\n",
       "         ('the', 'Tuileries'): 3,\n",
       "         ('Tuileries', 'for'): 1,\n",
       "         ('for', 'ye'): 3,\n",
       "         ('ye', '!'): 13,\n",
       "         ('!', 'But'): 24,\n",
       "         ('But', 'gulp'): 1,\n",
       "         ('gulp', 'down'): 2,\n",
       "         ('down', 'your'): 2,\n",
       "         ('your', 'tears'): 1,\n",
       "         ('tears', 'and'): 1,\n",
       "         ('and', 'hie'): 1,\n",
       "         ('hie', 'aloft'): 1,\n",
       "         ('aloft', 'to'): 4,\n",
       "         ('to', 'the'): 712,\n",
       "         ('the', 'royal'): 6,\n",
       "         ('royal', '-'): 2,\n",
       "         ('-', 'mast'): 29,\n",
       "         ('mast', 'with'): 2,\n",
       "         ('with', 'your'): 16,\n",
       "         ('your', 'hearts'): 3,\n",
       "         ('hearts', ';'): 2,\n",
       "         ('for', 'your'): 8,\n",
       "         ('your', 'friends'): 2,\n",
       "         ('friends', 'who'): 1,\n",
       "         ('who', 'have'): 17,\n",
       "         ('gone', 'before'): 3,\n",
       "         ('before', 'are'): 1,\n",
       "         ('are', 'clearing'): 1,\n",
       "         ('clearing', 'out'): 1,\n",
       "         ('out', 'the'): 40,\n",
       "         ('the', 'seven'): 5,\n",
       "         ('seven', '-'): 2,\n",
       "         ('-', 'storied'): 1,\n",
       "         ('storied', 'heavens'): 1,\n",
       "         ('heavens', ','): 2,\n",
       "         ('and', 'making'): 4,\n",
       "         ('making', 'refugees'): 1,\n",
       "         ('refugees', 'of'): 1,\n",
       "         ('of', 'long'): 2,\n",
       "         ('long', '-'): 11,\n",
       "         ('-', 'pampered'): 1,\n",
       "         ('pampered', 'Gabriel'): 1,\n",
       "         ('Gabriel', ','): 5,\n",
       "         (',', 'Michael'): 1,\n",
       "         ('Michael', ','): 1,\n",
       "         ('and', 'Raphael'): 1,\n",
       "         ('Raphael', ','): 1,\n",
       "         (',', 'against'): 6,\n",
       "         ('against', 'your'): 1,\n",
       "         ('your', 'coming'): 1,\n",
       "         ('coming', '.'): 4,\n",
       "         ('.', 'Here'): 25,\n",
       "         ('Here', 'ye'): 1,\n",
       "         ('ye', 'strike'): 1,\n",
       "         ('strike', 'but'): 1,\n",
       "         ('but', 'splintered'): 1,\n",
       "         ('splintered', 'hearts'): 1,\n",
       "         ('hearts', 'together'): 1,\n",
       "         ('together', '--'): 1,\n",
       "         ('--', 'there'): 11,\n",
       "         ('there', ','): 111,\n",
       "         (',', 'ye'): 61,\n",
       "         ('ye', 'shall'): 3,\n",
       "         ('shall', 'strike'): 1,\n",
       "         ('strike', 'unsplinterable'): 1,\n",
       "         ('unsplinterable', 'glasses'): 1,\n",
       "         ('glasses', '!'): 1,\n",
       "         ('!', 'EXTRACTS'): 1,\n",
       "         ('EXTRACTS', '.'): 1,\n",
       "         ('\"', 'And'): 32,\n",
       "         ('And', 'God'): 2,\n",
       "         ('God', 'created'): 1,\n",
       "         ('created', 'great'): 1,\n",
       "         ('great', 'whales'): 6,\n",
       "         ('whales', '.\"'): 2,\n",
       "         ('--', 'GENESIS'): 1,\n",
       "         ('GENESIS', '.'): 1,\n",
       "         ('\"', 'Leviathan'): 1,\n",
       "         ('Leviathan', 'maketh'): 1,\n",
       "         ('maketh', 'a'): 2,\n",
       "         ('a', 'path'): 1,\n",
       "         ('path', 'to'): 2,\n",
       "         ('to', 'shine'): 1,\n",
       "         ('shine', 'after'): 1,\n",
       "         ('after', 'him'): 7,\n",
       "         ('him', ';'): 88,\n",
       "         (';', 'One'): 1,\n",
       "         ('One', 'would'): 1,\n",
       "         ('would', 'think'): 8,\n",
       "         ('think', 'the'): 2,\n",
       "         ('the', 'deep'): 28,\n",
       "         ('deep', 'to'): 1,\n",
       "         ('be', 'hoary'): 1,\n",
       "         ('hoary', '.\"'): 1,\n",
       "         ('--', 'JOB'): 1,\n",
       "         ('JOB', '.'): 1,\n",
       "         ('\"', 'Now'): 10,\n",
       "         ('Now', 'the'): 5,\n",
       "         ('the', 'Lord'): 18,\n",
       "         ('Lord', 'had'): 1,\n",
       "         ('had', 'prepared'): 2,\n",
       "         ('prepared', 'a'): 3,\n",
       "         ('a', 'great'): 55,\n",
       "         ('great', 'fish'): 3,\n",
       "         ('fish', 'to'): 2,\n",
       "         ('to', 'swallow'): 3,\n",
       "         ('swallow', 'up'): 2,\n",
       "         ('up', 'Jonah'): 2,\n",
       "         ('Jonah', '.\"'): 2,\n",
       "         ('--', 'JONAH'): 1,\n",
       "         ('JONAH', '.'): 1,\n",
       "         ('\"', 'There'): 27,\n",
       "         ('There', 'go'): 5,\n",
       "         ('go', 'the'): 4,\n",
       "         ('the', 'ships'): 10,\n",
       "         ('ships', ';'): 4,\n",
       "         (';', 'there'): 22,\n",
       "         ('there', 'is'): 56,\n",
       "         ('is', 'that'): 33,\n",
       "         ('that', 'Leviathan'): 2,\n",
       "         ('Leviathan', 'whom'): 1,\n",
       "         ('whom', 'thou'): 1,\n",
       "         ('thou', 'hast'): 8,\n",
       "         ('hast', 'made'): 1,\n",
       "         ('made', 'to'): 13,\n",
       "         ('to', 'play'): 2,\n",
       "         ('play', 'therein'): 1,\n",
       "         ('therein', '.\"'): 1,\n",
       "         ('--', 'PSALMS'): 1,\n",
       "         ('PSALMS', '.'): 2,\n",
       "         ('\"', 'In'): 17,\n",
       "         ('In', 'that'): 7,\n",
       "         ('that', 'day'): 8,\n",
       "         ('day', ','): 43,\n",
       "         ('Lord', 'with'): 1,\n",
       "         ('with', 'his'): 111,\n",
       "         ('his', 'sore'): 1,\n",
       "         ('sore', ','): 1,\n",
       "         ('and', 'great'): 4,\n",
       "         ('great', ','): 7,\n",
       "         ('and', 'strong'): 4,\n",
       "         ('strong', 'sword'): 1,\n",
       "         ('sword', ','): 8,\n",
       "         (',', 'shall'): 5,\n",
       "         ('shall', 'punish'): 1,\n",
       "         ('punish', 'Leviathan'): 1,\n",
       "         ('Leviathan', 'the'): 1,\n",
       "         ('the', 'piercing'): 1,\n",
       "         ('piercing', 'serpent'): 1,\n",
       "         ('serpent', ','): 1,\n",
       "         (',', 'even'): 50,\n",
       "         ('even', 'Leviathan'): 1,\n",
       "         ('Leviathan', 'that'): 3,\n",
       "         ('that', 'crooked'): 1,\n",
       "         ('crooked', 'serpent'): 1,\n",
       "         ('serpent', ';'): 1,\n",
       "         ('and', 'he'): 49,\n",
       "         ('he', 'shall'): 3,\n",
       "         ('shall', 'slay'): 1,\n",
       "         ('slay', 'the'): 1,\n",
       "         ('the', 'dragon'): 1,\n",
       "         ('dragon', 'that'): 1,\n",
       "         ('that', 'is'): 76,\n",
       "         ('is', 'in'): 29,\n",
       "         ('in', 'the'): 1120,\n",
       "         ('the', 'sea'): 223,\n",
       "         ('sea', '.\"'): 4,\n",
       "         ('--', 'ISAIAH'): 1,\n",
       "         ('ISAIAH', '\"'): 1,\n",
       "         ('And', 'what'): 21,\n",
       "         ('what', 'thing'): 2,\n",
       "         ('thing', 'soever'): 1,\n",
       "         ('soever', 'besides'): 1,\n",
       "         ('besides', 'cometh'): 1,\n",
       "         ('cometh', 'within'): 1,\n",
       "         ('within', 'the'): 19,\n",
       "         ('the', 'chaos'): 1,\n",
       "         ('chaos', 'of'): 1,\n",
       "         ('this', 'monster'): 4,\n",
       "         ('monster', \"'\"): 6,\n",
       "         ('s', 'mouth'): 9,\n",
       "         ('mouth', ','): 22,\n",
       "         (',', 'be'): 32,\n",
       "         ('be', 'it'): 24,\n",
       "         ('it', 'beast'): 1,\n",
       "         ('beast', ','): 1,\n",
       "         (',', 'boat'): 1,\n",
       "         ('boat', ','): 78,\n",
       "         (',', 'or'): 257,\n",
       "         ('or', 'stone'): 1,\n",
       "         ('stone', ','): 4,\n",
       "         (',', 'down'): 10,\n",
       "         ('down', 'it'): 2,\n",
       "         ('it', 'goes'): 3,\n",
       "         ('goes', 'all'): 1,\n",
       "         ('all', 'incontinently'): 1,\n",
       "         ('incontinently', 'that'): 1,\n",
       "         ('that', 'foul'): 1,\n",
       "         ('foul', 'great'): 1,\n",
       "         ('great', 'swallow'): 1,\n",
       "         ('swallow', 'of'): 1,\n",
       "         ('his', ','): 15,\n",
       "         ('and', 'perisheth'): 1,\n",
       "         ('perisheth', 'in'): 1,\n",
       "         ('the', 'bottomless'): 5,\n",
       "         ('bottomless', 'gulf'): 1,\n",
       "         ('gulf', 'of'): 3,\n",
       "         ('his', 'paunch'): 1,\n",
       "         ('paunch', '.\"'): 1,\n",
       "         ('--', 'HOLLAND'): 2,\n",
       "         ('HOLLAND', \"'\"): 2,\n",
       "         ('S', 'PLUTARCH'): 1,\n",
       "         ('PLUTARCH', \"'\"): 1,\n",
       "         ('S', 'MORALS'): 1,\n",
       "         ('MORALS', '.'): 1,\n",
       "         ('\"', 'The'): 59,\n",
       "         ('The', 'Indian'): 1,\n",
       "         ('Indian', 'Sea'): 1,\n",
       "         ('Sea', 'breedeth'): 1,\n",
       "         ('breedeth', 'the'): 1,\n",
       "         ('the', 'most'): 110,\n",
       "         ('most', 'and'): 1,\n",
       "         ('the', 'biggest'): 1,\n",
       "         ('biggest', 'fishes'): 1,\n",
       "         ('fishes', 'that'): 1,\n",
       "         ('that', 'are'): 6,\n",
       "         ('are', ':'): 1,\n",
       "         (':', 'among'): 1,\n",
       "         ('among', 'which'): 4,\n",
       "         ('which', 'the'): 69,\n",
       "         ('the', 'Whales'): 2,\n",
       "         ('Whales', 'and'): 2,\n",
       "         ('and', 'Whirlpooles'): 1,\n",
       "         ('Whirlpooles', 'called'): 1,\n",
       "         ('called', 'Balaene'): 1,\n",
       "         ('Balaene', ','): 1,\n",
       "         ('take', 'up'): 2,\n",
       "         ('up', 'as'): 4,\n",
       "         ('as', 'much'): 23,\n",
       "         ('much', 'in'): 7,\n",
       "         ('in', 'length'): 17,\n",
       "         ('length', 'as'): 2,\n",
       "         ('as', 'four'): 1,\n",
       "         ('four', 'acres'): 1,\n",
       "         ('acres', 'or'): 1,\n",
       "         ('or', 'arpens'): 1,\n",
       "         ('arpens', 'of'): 1,\n",
       "         ('of', 'land'): 13,\n",
       "         ('land', '.\"'): 1,\n",
       "         ('S', 'PLINY'): 1,\n",
       "         ('PLINY', '.'): 1,\n",
       "         ('\"', 'Scarcely'): 1,\n",
       "         ('Scarcely', 'had'): 1,\n",
       "         ('had', 'we'): 1,\n",
       "         ('we', 'proceeded'): 1,\n",
       "         ('proceeded', 'two'): 1,\n",
       "         ('two', 'days'): 4,\n",
       "         ('days', 'on'): 1,\n",
       "         ('on', 'the'): 326,\n",
       "         ('sea', ','): 104,\n",
       "         (',', 'when'): 238,\n",
       "         ('when', 'about'): 3,\n",
       "         ('about', 'sunrise'): 1,\n",
       "         ('sunrise', 'a'): 1,\n",
       "         ('great', 'many'): 1,\n",
       "         ('many', 'Whales'): 1,\n",
       "         ('and', 'other'): 15,\n",
       "         ('other', 'monsters'): 1,\n",
       "         ('monsters', 'of'): 2,\n",
       "         (',', 'appeared'): 1,\n",
       "         ('appeared', '.'): 1,\n",
       "         ('.', 'Among'): 7,\n",
       "         ('Among', 'the'): 4,\n",
       "         ('the', 'former'): 6,\n",
       "         ('former', ','): 1,\n",
       "         (',', 'one'): 54,\n",
       "         ('one', 'was'): 4,\n",
       "         ('was', 'of'): 11,\n",
       "         ('a', 'most'): 25,\n",
       "         ('most', 'monstrous'): 2,\n",
       "         ('monstrous', 'size'): 2,\n",
       "         ('size', '.'): 1,\n",
       "         ('...', 'This'): 1,\n",
       "         ('This', 'came'): 1,\n",
       "         ('came', 'towards'): 1,\n",
       "         ('towards', 'us'): 3,\n",
       "         ('us', ','): 36,\n",
       "         (',', 'open'): 4,\n",
       "         ('open', '-'): 6,\n",
       "         ('-', 'mouthed'): 4,\n",
       "         ('mouthed', ','): 1,\n",
       "         (',', 'raising'): 2,\n",
       "         ('raising', 'the'): 1,\n",
       "         ('the', 'waves'): 21,\n",
       "         ('waves', 'on'): 1,\n",
       "         ('on', 'all'): 11,\n",
       "         ('all', 'sides'): 7,\n",
       "         ('sides', ','): 9,\n",
       "         ('and', 'beating'): 1,\n",
       "         ('beating', 'the'): 1,\n",
       "         ('sea', 'before'): 2,\n",
       "         ('before', 'him'): 31,\n",
       "         ('him', 'into'): 4,\n",
       "         ('into', 'a'): 42,\n",
       "         ('a', 'foam'): 1,\n",
       "         ('foam', '.\"'): 1,\n",
       "         ('--', 'TOOKE'): 1,\n",
       "         ('TOOKE', \"'\"): 1,\n",
       "         ('S', 'LUCIAN'): 1,\n",
       "         ('LUCIAN', '.'): 1,\n",
       "         ('\"', 'THE'): 2,\n",
       "         ('THE', 'TRUE'): 1,\n",
       "         ('TRUE', 'HISTORY'): 1,\n",
       "         ('HISTORY', '.\"'): 1,\n",
       "         ('.\"', '\"'): 246,\n",
       "         ('\"', 'He'): 29,\n",
       "         ('He', 'visited'): 1,\n",
       "         ('visited', 'this'): 2,\n",
       "         ('this', 'country'): 1,\n",
       "         ('country', 'also'): 1,\n",
       "         ('also', 'with'): 1,\n",
       "         ('a', 'view'): 5,\n",
       "         ('of', 'catching'): 1,\n",
       "         ('catching', 'horse'): 1,\n",
       "         ('horse', '-'): 9,\n",
       "         ('-', 'whales'): 1,\n",
       "         ('whales', ','): 63,\n",
       "         ('which', 'had'): 18,\n",
       "         ('had', 'bones'): 1,\n",
       "         ('bones', 'of'): 13,\n",
       "         ('of', 'very'): 1,\n",
       "         ('very', 'great'): 2,\n",
       "         ('great', 'value'): 1,\n",
       "         ('value', 'for'): 1,\n",
       "         ('for', 'their'): 15,\n",
       "         ('their', 'teeth'): 2,\n",
       "         ('teeth', ','): 18,\n",
       "         (',', 'of'): 62,\n",
       "         ('of', 'which'): 34,\n",
       "         ('which', 'he'): 43,\n",
       "         ('he', 'brought'): 1,\n",
       "         ('brought', 'some'): 1,\n",
       "         ('some', 'to'): 1,\n",
       "         ('the', 'king'): 7,\n",
       "         ('king', '.'): 2,\n",
       "         ('...', 'The'): 1,\n",
       "         ('The', 'best'): 2,\n",
       "         ('best', 'whales'): 1,\n",
       "         ('whales', 'were'): 10,\n",
       "         ('were', 'catched'): 1,\n",
       "         ('catched', 'in'): 1,\n",
       "         ('in', 'his'): 253,\n",
       "         ('his', 'own'): 103,\n",
       "         ('own', 'country'): 1,\n",
       "         ('country', ','): 2,\n",
       "         ('which', 'some'): 4,\n",
       "         ('some', 'were'): 2,\n",
       "         ('were', 'forty'): 1,\n",
       "         ('forty', '-'): 4,\n",
       "         ('-', 'eight'): 3,\n",
       "         ('eight', ','): 2,\n",
       "         (',', 'some'): 47,\n",
       "         ('some', 'fifty'): 3,\n",
       "         ('fifty', 'yards'): 3,\n",
       "         ('yards', 'long'): 1,\n",
       "         ('long', '.'): 7,\n",
       "         ('He', 'said'): 3,\n",
       "         ('said', 'that'): 9,\n",
       "         ('that', 'he'): 98,\n",
       "         ('he', 'was'): 114,\n",
       "         ('was', 'one'): 13,\n",
       "         ('one', 'of'): 144,\n",
       "         ('of', 'six'): 2,\n",
       "         ('six', 'who'): 1,\n",
       "         ('who', 'had'): 26,\n",
       "         ('had', 'killed'): 4,\n",
       "         ('killed', 'sixty'): 1,\n",
       "         ('sixty', 'in'): 1,\n",
       "         ('in', 'two'): 9,\n",
       "         ('days', '.\"'): 1,\n",
       "         ('--', 'OTHER'): 1,\n",
       "         ('OTHER', 'OR'): 1,\n",
       "         ('OR', 'OCTHER'): 1,\n",
       "         ('OCTHER', \"'\"): 1,\n",
       "         ('S', 'VERBAL'): 1,\n",
       "         ('VERBAL', 'NARRATIVE'): 1,\n",
       "         ('NARRATIVE', 'TAKEN'): 1,\n",
       "         ('TAKEN', 'DOWN'): 1,\n",
       "         ('DOWN', 'FROM'): 1,\n",
       "         ('FROM', 'HIS'): 1,\n",
       "         ('HIS', 'MOUTH'): 1,\n",
       "         ('MOUTH', 'BY'): 1,\n",
       "         ('BY', 'KING'): 1,\n",
       "         ('KING', 'ALFRED'): 1,\n",
       "         ('ALFRED', ','): 1,\n",
       "         (',', 'A'): 12,\n",
       "         ('.', 'D'): 15,\n",
       "         ('D', '.'): 12,\n",
       "         ('.', '890'): 1,\n",
       "         ('890', '.'): 1,\n",
       "         ('And', 'whereas'): 1,\n",
       "         ('whereas', 'all'): 1,\n",
       "         ('the', 'other'): 135,\n",
       "         ('other', 'things'): 8,\n",
       "         ('things', ','): 26,\n",
       "         (',', 'whether'): 21,\n",
       "         ('whether', 'beast'): 1,\n",
       "         ('beast', 'or'): 1,\n",
       "         ('or', 'vessel'): 1,\n",
       "         ('vessel', ','): 9,\n",
       "         (',', 'that'): 584,\n",
       "         ('that', 'enter'): 1,\n",
       "         ('enter', 'into'): 3,\n",
       "         ('into', 'the'): 246,\n",
       "         ('the', 'dreadful'): 1,\n",
       "         ('dreadful', 'gulf'): 1,\n",
       "         ('s', '('): 4,\n",
       "         ('(', 'whale'): 1,\n",
       "         ('whale', \"'\"): 81,\n",
       "         ('s', ')'): 3,\n",
       "         (')', 'mouth'): 1,\n",
       "         (',', 'are'): 41,\n",
       "         ('are', 'immediately'): 1,\n",
       "         ('immediately', 'lost'): 1,\n",
       "         ('lost', 'and'): 2,\n",
       "         ('and', 'swallowed'): 4,\n",
       "         ('swallowed', 'up'): 4,\n",
       "         ('sea', '-'): 66,\n",
       "         ('-', 'gudgeon'): 1,\n",
       "         ('gudgeon', 'retires'): 1,\n",
       "         ('retires', 'into'): 1,\n",
       "         ('into', 'it'): 15,\n",
       "         ('it', 'in'): 31,\n",
       "         ('in', 'great'): 8,\n",
       "         ('great', 'security'): 1,\n",
       "         ('security', ','): 1,\n",
       "         ('and', 'there'): 42,\n",
       "         ('there', 'sleeps'): 1,\n",
       "         ('sleeps', '.\"'): 1,\n",
       "         ('--', 'MONTAIGNE'): 1,\n",
       "         ('MONTAIGNE', '.'): 1,\n",
       "         ('.', '--'): 1,\n",
       "         ('--', 'APOLOGY'): 1,\n",
       "         ('APOLOGY', 'FOR'): 1,\n",
       "         ('FOR', 'RAIMOND'): 1,\n",
       "         ('RAIMOND', 'SEBOND'): 1,\n",
       "         ('SEBOND', '.'): 1,\n",
       "         ('\"', 'Let'): 3,\n",
       "         ('Let', 'us'): 7,\n",
       "         ('us', 'fly'): 3,\n",
       "         ('fly', ','): 2,\n",
       "         (',', 'let'): 28,\n",
       "         ('let', 'us'): 20,\n",
       "         ('fly', '!'): 1,\n",
       "         ('!', 'Old'): 1,\n",
       "         ('Old', 'Nick'): 1,\n",
       "         ('Nick', 'take'): 1,\n",
       "         ('take', 'me'): 2,\n",
       "         ('me', 'if'): 5,\n",
       "         ('if', 'is'): 1,\n",
       "         ('not', 'Leviathan'): 1,\n",
       "         ('Leviathan', 'described'): 1,\n",
       "         ('described', 'by'): 1,\n",
       "         ('by', 'the'): 301,\n",
       "         ('the', 'noble'): 8,\n",
       "         ('noble', 'prophet'): 1,\n",
       "         ('prophet', 'Moses'): 1,\n",
       "         ('Moses', 'in'): 1,\n",
       "         ('the', 'life'): 14,\n",
       "         ('life', 'of'): 13,\n",
       "         ('of', 'patient'): 1,\n",
       "         ('patient', 'Job'): 1,\n",
       "         ('Job', '.\"'): 1,\n",
       "         ('--', 'RABELAIS'): 1,\n",
       "         ('RABELAIS', '.'): 1,\n",
       "         ('\"', 'This'): 3,\n",
       "         ('This', 'whale'): 5,\n",
       "         ('s', 'liver'): 1,\n",
       "         ('liver', 'was'): 1,\n",
       "         ('was', 'two'): 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moby = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "count = collections.Counter(nltk.bigrams(moby))\n",
    "d = [[w,n] for [w,n] in count if w == \"Moby\"]\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Preprocessing with NLTK\n",
    "The following exercises will ask several questions about tokens, stems, and parts of speech.\n",
    "\n",
    "### Exercise 2.1\n",
    "*What is the sentence with the largest number of tokens in Austen's \"Emma\"?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1341"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "nltk.sent_tokenize(text)\n",
    "\n",
    "e = [len(s) for s in nltk.sent_tokenize(text)]\n",
    "max(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "*What is the most frequent part of speech in Austen's \"Emma\"?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 19346), ('IN', 17877), ('PRP', 15600), ('RB', 12985), ('DT', 12738)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "*What is the number of distinct stems in Austen's \"Emma\"?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austen's Emma has 5450 distinct stems\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4\n",
    "*What is the most ambiguous stem in Austen's \"Emma\"? (meaning, which stem in Austen's \"Emma\" maps to the largest number of distinct tokens?)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most ambiguous stem is respect with words {'respectfully', 'respectable', 'respecting', 'respectability', 'Respect', 'respectful', 'respectably', 'respected', 'respect', 'respects', 'respective'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regular Expressions\n",
    "The aim of this section is to develop a regular expression that detects all the numerical expressions in the Brown corpus. The Brown corpus is described in the [NLTK book chapter 2](http://www.nltk.org/book_1ed/ch02.html), section 2.1.\n",
    "\n",
    "The Brown corpus is a corpus annotated with the parts of speech. In the following exercises, you will concentrate on the words tagged with labels that begin with the `CD` tag. This tag stands for \"cardinal numeral\" and indicates that the token is a number. For the full list of tags in the Brown corpus you can see the [Wikipedia entry](https://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used).\n",
    "\n",
    "The following Python code shows the first 5 words of the \"news\" category of the Brown corpus. You'll see that it is a list of pairs, where each pair is a word and a tag. The tag has two components separated with `-`. We will focus on the labels that begin with `'CD'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/Phillip/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('brown')\n",
    "tagged = nltk.corpus.brown.tagged_words(categories='news')\n",
    "tagged[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code stores the list of unannotated tokens in the variable `tokens`. Note that we could have used `nltk.corpus.brown.words` but build this list but the code below makes sure that the list mirrors the list `tagged'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [w for (w,t) in tagged]\n",
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "*Write code that finds all the numbers (items tagged with `'CD'`) from the list \"`tagged`\" and stores them in a new variable \"`numbers`\". How many numbers are there? How many different numbers are there?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n",
      "460\n"
     ]
    }
   ],
   "source": [
    "numbers = [w for (w,t) in tagged if t == \"CD\"]\n",
    "print (len(numbers))\n",
    "print (len(set(numbers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "*Complete the following function that finds all the items in `tokens` that match the following regular expression: `^[0-9]+$`. The function should return the result as a list of pairs `(word,annotation)` so that, if the word is a number, the annotation is `'CD'`, and if it is not a number, the annotation is `''`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def annotateNum(listtokens):\n",
    "    \"\"\"Annotate the list of tokens to identify the numbers.\n",
    "    Example of run:\n",
    "    >>> annotateNum(['the','number','5'])\n",
    "    [('the', ''), ('number', ''), ('5', 'CD')]\n",
    "    \"\"\"\n",
    "    # Write your code here and edit the \"return\" statement\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', ''), ('number', ''), ('5', 'CD')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotateNum(['the','number','5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code computes the recall and precision of the annotations.\n",
    "\n",
    "1. The *recall* is the ratio of numbers that are tagged correctly.\n",
    "2. The *precision* is the ratio of tagged tokens that are numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(result,tagged):\n",
    "    assert len(result) == len(tagged) # This is a check that the length of the result and tagged are equal\n",
    "    correct = [result[i][0] for i in range(len(result)) if result[i][1][:2] == 'CD' and tagged[i][1][:2] == 'CD']\n",
    "    numbers_result = [result[i][0] for i in range(len(result)) if result[i][1][:2] == 'CD']\n",
    "    numbers_tagged = [tagged[i][0] for i in range(len(tagged)) if tagged[i][1][:2] == 'CD']\n",
    "    print(\"Recall:\",len(correct)/len(numbers_tagged))\n",
    "    print(\"Precision:\",len(correct)/len(numbers_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5304428044280443\n",
      "Precision: 0.9982638888888888\n"
     ]
    }
   ],
   "source": [
    "evaluate(annotateNum(tokens),tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code returns the mistakes that you produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def false_positives(result,tagged):\n",
    "    \"Return the non-numbers that were tagged as numbers\"\n",
    "    return [tagged[i] for i in range(len(result)) if result[i][1]== 'CD' and tagged[i][1][:2] != 'CD']\n",
    "def false_negatives(result,tagged):\n",
    "    \"Return the numbers that were not tagged as numbers\"\n",
    "    return [result[i] for i in range(len(result)) if result[i][1]!= 'CD' and tagged[i][1][:2] == 'CD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3', 'OD'), ('3', 'OD-TL')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positives(annotateNum(tokens),tagged)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('two', ''),\n",
       " ('one', ''),\n",
       " ('two', ''),\n",
       " ('Four', ''),\n",
       " ('one', ''),\n",
       " ('one', ''),\n",
       " ('one', ''),\n",
       " ('two', ''),\n",
       " ('Five', ''),\n",
       " ('three', '')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives(annotateNum(tokens),tagged)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3\n",
    "**This exercise is optional for this workshop but it will be very useful for the sort of work that you would normally do as a developer of text applications. This kind of workflow can also be useful for the assignments.**\n",
    "\n",
    "*Concentrate on your worst figure, either recall and precision, and try to improve it:*\n",
    "\n",
    "1. *If recall is low, list all the numbers that your system failed to detect (the false negatives). With those numbers in mind, update your regular expression.*\n",
    "2. *If precision is low, list all the tokens that your system erroneously classified as numbers (the false positives). The update your regular expression to cover those words.*\n",
    "\n",
    "*Test your new regular expression on the full Brown corpus, compute recall and precision, and see what you can improve. Do this several times and try to get better results.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Solution: There's no single best pattern. The main goal of this activity is to\n",
    "help the students develop their analytical skills and to find a good\n",
    "methodology to develop rules. They could do this exercise in groups\n",
    "of two, and they should *really* test their patterns by computing\n",
    "recall and precision. I haven't introduced the concepts of recall and\n",
    "precision in the lectures yet, but hopefully they can see the usefulness\n",
    "of these measures for the development of rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
